{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1afb2ec3-c35b-4f2f-b689-372194f6dd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_best_lstm_model(best_params):\n",
    "    model = Sequential()\n",
    "    n_layers = best_params['lstm_n_layers']\n",
    "    units = best_params['lstm_units']\n",
    "    for i in range(n_layers):\n",
    "        model.add(LSTM(\n",
    "            units=units,\n",
    "            return_sequences=(i < n_layers - 1),\n",
    "            input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2]) if i == 0 else None\n",
    "        ))\n",
    "        model.add(Dropout(rate=best_params['lstm_dropout']))\n",
    "    model.add(Dense(1))\n",
    "    learning_rate = best_params['lstm_learning_rate']\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "def create_lstm_model(trial):\n",
    "    model = Sequential()\n",
    "    n_layers = trial.suggest_int('lstm_n_layers', 2, 4)  # Optimize number of LSTM layers\n",
    "    units = trial.suggest_categorical('lstm_units', [50, 100, 150, 200])\n",
    "    for i in range(n_layers):\n",
    "        model.add(LSTM(\n",
    "            units=units,\n",
    "            return_sequences=(i < n_layers - 1),\n",
    "            input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2]) if i == 0 else None\n",
    "        ))\n",
    "        dropout_rate = trial.suggest_categorical('lstm_dropout', [0.1, 0.2, 0.3, 0.4, 0.5])\n",
    "        model.add(Dropout(rate=dropout_rate))  # Use specific dropout rate\n",
    "    model.add(Dense(1))\n",
    "    learning_rate = trial.suggest_categorical('lstm_learning_rate', [0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001])  # Optimize learning rate\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Define optimization objective function\n",
    "def objective_LSTM(trial):\n",
    "    num_samples = X_train_lstm.shape[0] * 0.8\n",
    "    batch_size_candidates = [16, 32, 64, 128, 256, 512, 1024, 2048]\n",
    "    batch_size_candidates = [bs for bs in batch_size_candidates if bs <= num_samples]\n",
    "    param = {\n",
    "        'epochs': trial.suggest_categorical('lstm_epochs', [10, 20, 30, 40, 50, 75, 100, 150, 200]),\n",
    "        'batch_size': trial.suggest_categorical('lstm_batch_size', batch_size_candidates)\n",
    "    }\n",
    "    param['batch_size'] = min(param['batch_size'], num_samples)  # Ensure batch_size does not exceed the number of samples in the training dataset\n",
    "    model = KerasRegressor(build_fn=lambda: create_lstm_model(trial), epochs=param['epochs'], batch_size=param['batch_size'], verbose=0)\n",
    "    \n",
    "    # Use 5-fold cross-validation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    mse_scores = []\n",
    "    \n",
    "    for train_index, val_index in kf.split(X_train_lstm):\n",
    "        X_train, X_val = X_train_lstm[train_index], X_train_lstm[val_index]\n",
    "        y_train_fold, y_val = y_train[train_index], y_train[val_index]\n",
    "        num_samples = X_train.shape[0]\n",
    "        # 'patience' parameter defines how many epochs the model continues training without improvement before stopping\n",
    "        # 'restore_best_weights' if True, the model's weights will be restored to the point with the lowest validation loss\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train_fold, validation_data=(X_val, y_val), callbacks=[early_stopping, KerasPruningCallback(trial, 'val_loss')])\n",
    "        \n",
    "        y_pred = model.predict(X_val)\n",
    "        mse = mean_squared_error(y_val, y_pred)\n",
    "        mse_scores.append(mse)\n",
    "    \n",
    "    return np.mean(mse_scores)\n",
    "\n",
    "def plt_scatter(data, filename):\n",
    "    dataframes = data[['predicts', 'records']]\n",
    "    dataframes1 = dataframes\n",
    "    r2 = r2_score(dataframes['records'], dataframes['predicts'])\n",
    "    # Calculate NRMSE\n",
    "    mse = mean_squared_error(dataframes['records'], dataframes['predicts'])\n",
    "    nrmse = np.sqrt(mse) / np.mean(dataframes['records'])\n",
    "    rrmse = calculate_rrmse1(dataframes['records'], dataframes['predicts'])\n",
    "    \n",
    "    mape = mean_absolute_percentage_error(dataframes['records'], dataframes['predicts'])\n",
    "    acc = calculate_acc(dataframes['records'], dataframes['predicts'])\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hexbin(dataframes['records'], dataframes['predicts'], gridsize=50, cmap='viridis', mincnt=1)\n",
    "    cb = plt.colorbar(label='Density')\n",
    "    fit_params = np.polyfit(dataframes['records'], dataframes['predicts'], 1)\n",
    "    fit_line = np.polyval(fit_params, dataframes['records'])\n",
    "    plt.plot(dataframes['records'], fit_line, color='red', label='Fit line')  # Add fitting line\n",
    "    max_val = max(max(dataframes['records']), max(dataframes['predicts']))\n",
    "    plt.plot([0, max_val], [0, max_val], linestyle='--', color='green', label='1:1 line')\n",
    "    plt.title('')\n",
    "    plt.xlabel('Observed values')\n",
    "    plt.ylabel('Predicted values')\n",
    "    plt.grid(True)\n",
    "    plt.text(0.02, 0.95, f'RÂ² = {r2:.2f}', transform=plt.gca().transAxes, fontsize=12, va='top')\n",
    "    plt.text(0.02, 0.90, f'ACC = {acc:.2f}', transform=plt.gca().transAxes, fontsize=12, va='top')\n",
    "    plt.text(0.02, 0.85, f'RRMSE = {rrmse:.2f}%', transform=plt.gca().transAxes, fontsize=12, va='top')\n",
    "    plt.text(0.02, 0.80, f'MAPE = {mape*100:.2f}%', transform=plt.gca().transAxes, fontsize=12, va='top')\n",
    "    \n",
    "    # Display and save the figure\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "def extract_selected_variables(inputpath_base):\n",
    "    inpath_dates = os.path.join(inputpath_base, 'dataset', \"01_inputdata\", 'selectFeatures.txt')\n",
    "    # Construct file path\n",
    "    gs_infornamtion = pd.read_csv(inpath_dates, sep='\\t', header=None)\n",
    "    gs_infornamtion.columns = ['slected_dynamic_features', 'slected_static', 'regionID']\n",
    "    gs_infornamtion['slected_dynamic_features'] = gs_infornamtion['slected_dynamic_features'].apply(ast.literal_eval)\n",
    "    gs_infornamtion['slected_static'] = gs_infornamtion['slected_static'].apply(ast.literal_eval)\n",
    "    return gs_infornamtion\n",
    "\n",
    "def calculate_rrmse1(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate RRMSE (Relative Root Mean Square Error) using the mean of actual values as reference.\n",
    "    \n",
    "    Parameters:\n",
    "    y_true -- Array or list of true values\n",
    "    y_pred -- Array or list of predicted values\n",
    "    \n",
    "    Returns:\n",
    "    rrmse -- Relative Root Mean Square Error (percentage)\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "    \n",
    "    # Calculate mean of true values\n",
    "    mean_y_true = np.mean(y_true)\n",
    "    \n",
    "    # Calculate RRMSE\n",
    "    rrmse = (rmse / mean_y_true) * 100\n",
    "    \n",
    "    return rrmse\n",
    "\n",
    "def calculate_rrmse2(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate rRMSE (Relative Root Mean Square Error) using each individual actual value as reference.\n",
    "    \n",
    "    Parameters:\n",
    "    y_true -- Array or list of true values\n",
    "    y_pred -- Array or list of predicted values\n",
    "    \n",
    "    Returns:\n",
    "    rrmse -- Relative Root Mean Square Error (percentage)\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # Calculate rRMSE\n",
    "    rrmse = np.sqrt(np.mean(((y_true - y_pred) / y_true) ** 2)) * 100\n",
    "    \n",
    "    return rrmse\n",
    "\n",
    "# Define custom nRMSE evaluation function\n",
    "def calculate_nrmse(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    nrmse = rmse / (y_true.max() - y_true.min())\n",
    "    return nrmse * 100\n",
    "\n",
    "def calculate_acc(y_true, y_pred):\n",
    "    # Calculate means of observed and predicted values\n",
    "    mean_true = np.mean(y_true)\n",
    "    mean_pred = np.mean(y_pred)\n",
    "    \n",
    "    # Calculate anomalies (deviations from mean)\n",
    "    anomaly_true = y_true - mean_true\n",
    "    anomaly_pred = y_pred - mean_pred\n",
    "    \n",
    "    # Calculate ACC (Anomaly Correlation Coefficient)\n",
    "    numerator = np.sum(anomaly_true * anomaly_pred)\n",
    "    denominator = np.sqrt(np.sum(anomaly_true**2) * np.sum(anomaly_pred**2))\n",
    "    \n",
    "    acc = numerator / denominator\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ca5fb3c-1540-4fea-9931-9911e8efbe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "sys.path.append(r'C:\\ProgramData\\anaconda3\\Lib\\site-packages') \n",
    "sys.path.append(r'C:\\Users\\DELL\\.conda\\envs\\myenv\\Lib\\site-packages') \n",
    "sys.path.append(r'C:\\Users\\DELL\\.conda\\envs\\rasterio_env\\Lib\\site-packages') \n",
    "import sys\n",
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from optuna_integration.keras import KerasPruningCallback\n",
    "from optuna.samplers import TPESampler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from tqdm.notebook import tqdm\n",
    "import tensorflow.keras.backend as K\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22a307d-52fc-42c5-8f4d-f9489bb9d16a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a744a2fd-5ba2-47cc-8fe5-3bb2b698e94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Required Packages to Install\n",
    "Pre-installed Packages:\n",
    "os, sys, warnings, optuna \n",
    "\n",
    "Packages to Install:\n",
    "matplotlib, sklearn, optuna_integration, tensorflow, scikeras, ast\n",
    "\n",
    "Installation Commands:\n",
    "conda install optuna matplotlib sklearn optuna_integration tensorflow scikeras ast -c pytorch\n",
    "conda install matplotlib tensorflow scikeras -c pytorch\n",
    "'''\n",
    "\n",
    "countryID = '06_India';  # Country identifier (format: XX_CountryName)\n",
    "country = countryID[3:];  # Country name extracted from countryID\n",
    "crop = '02_wheat';  # Crop type (format: XX_CropName)\n",
    "yield_type = 'actual_yield';  # Yield type to predict (e.g., actual yield, potential yield)\n",
    "\n",
    "inputpath_base = os.path.dirname(os.getcwd())  # Base input path (parent directory of current working directory)\n",
    "startyear = 2001;  # Start year of the dataset\n",
    "endyear = 2018;    # End year of the dataset\n",
    "Forecastyear = endyear;  # Forecast target year (same as endyear in this configuration)\n",
    "region = 'I';  # Region identifier (custom region code)\n",
    "Forecastyears = {\n",
    "    'I': Forecastyear  # Forecast years corresponding to each region (key: region ID, value: forecast year)\n",
    "}\n",
    "years = range(startyear, endyear + 1)  # Range of years covered in the dataset\n",
    "years_str = [str(year) for year in years]  # String format of dataset years (for file naming/path construction)\n",
    "modelname = 'LSTM';  # Machine learning model name (here: Long Short-Term Memory network)\n",
    "yield_type = 'actual_yield';  # Reconfirm yield type (consistent with earlier definition)\n",
    "SelFeature_infornamtion = extract_selected_variables(inputpath_base)  # Extract selected feature information (dynamic and static features)\n",
    "institution = 'ECMWF';  # Meteorological data provider (European Centre for Medium-Range Weather Forecasts)\n",
    "\n",
    "# Read selected growth stage week information\n",
    "inpath_dates = os.path.join(inputpath_base, 'dataset', \"01_inputdata\", 'gs_three_periods.txt')  # Path to growth stage date file\n",
    "gs_infornamtion = pd.read_csv(inpath_dates, delim_whitespace=True, header=None)  # Read growth stage data (tab-separated)\n",
    "gs_infornamtion.columns = ['start_point', 'peak', 'harvest_point', 'VI_select2', 'regionID']  # Rename columns: start point, peak stage, harvest point, selected VI, region ID\n",
    "# Extract growth stage parameters for the target region\n",
    "start_point, peak, harvest_point, VI_select2, region = gs_infornamtion[gs_infornamtion['regionID'] == region].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f8d32a-0a53-4cc5-a138-bcee2f9cc0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########Hyperparameter Tuning###########################################\n",
    "###################Load Selected Feature Variables#########################################\n",
    "Forecastyear = Forecastyears[region]  # Get forecast year for the target region\n",
    "# Load original dataset\n",
    "data = pd.read_csv(os.path.join(inputpath_base, 'dataset', \"01_inputdata\", region + '_data_ori.csv'))\n",
    "data = data.drop_duplicates(subset=['year', 'idJoin'], keep='first')  # Remove duplicate records (retain first occurrence)\n",
    "data = data[data['year'] != Forecastyear]  # Exclude data from the forecast year\n",
    "\n",
    "# Extract selected dynamic features, static features, and region ID\n",
    "TimeFeatures_sel, Static_sel, regionID = SelFeature_infornamtion[SelFeature_infornamtion['regionID'] == region].iloc[0]\n",
    "feature_all = TimeFeatures_sel + Static_sel  # Combine dynamic and static features into a complete feature list\n",
    "# Filter columns containing any of the selected features\n",
    "filtered_columns = [col for col in data.columns if any(feature in col for feature in feature_all)]\n",
    "filtered_columns = [col for col in filtered_columns if 'year.1' not in col]  # Exclude redundant year column ('year.1')\n",
    "\n",
    "####Exclude yield-related features (for correlation testing)#########################\n",
    "filtered_columns = [col for col in filtered_columns if 'Yield' not in col]  # Exclude yield-related features\n",
    "Static_sel = [col for col in Static_sel if 'Yield' not in col]  # Update static features (remove yield-related items)\n",
    "\n",
    "# Construct machine learning dataset (selected features + target yield)\n",
    "MLdata_reduced = data[filtered_columns + [yield_type]]\n",
    "MLdata_reduced['year'] = data['year']  # Add year column back to the dataset\n",
    "\n",
    "\n",
    "############################Data Standardization###########################################################\n",
    "data_all = MLdata_reduced  # Full dataset for modeling\n",
    "X_all = data_all.drop([yield_type], axis=1)  # Feature matrix (exclude target variable)\n",
    "y_all = data_all[yield_type]  # Target variable (yield)\n",
    "\n",
    "# Standardize feature matrix (Z-score normalization: mean=0, std=1)\n",
    "scaler_X = StandardScaler().fit(X_all)\n",
    "X = scaler_X.transform(X_all)\n",
    "# Standardize target variable\n",
    "scaler_y = StandardScaler().fit(y_all.values.reshape(-1, 1))\n",
    "y = scaler_y.transform(y_all.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Convert standardized features back to DataFrame (for easier column-based operations)\n",
    "X = pd.DataFrame(data=X, columns=X_all.columns.tolist())\n",
    "\n",
    "# Generate list of growth stage weeks (handle cross-year growth periods)\n",
    "if start_point > harvest_point:\n",
    "    # If start week > harvest week (cross-year growth, e.g., winter crops), combine two ranges\n",
    "    weeks_select_list = list(range(start_point, 47)) + list(range(1, harvest_point + 1))\n",
    "else:\n",
    "    # Normal growth period (same year), direct range from start to harvest week\n",
    "    weeks_select_list = list(range(start_point, harvest_point + 1))\n",
    "\n",
    "# Reshape data into 3D format (samples, time steps, features) required for LSTM\n",
    "data_list = []\n",
    "for i in weeks_select_list:\n",
    "    # Extract features for the i-th week (dynamic features + static features)\n",
    "    data_i = X[[f'Week{i}_{feature}' for feature in TimeFeatures_sel] + Static_sel]\n",
    "    data_list.append(data_i.values)  # Store weekly feature matrices in a list\n",
    "\n",
    "# Stack weekly feature matrices into a 3D array\n",
    "data_list = np.array(data_list)\n",
    "# Reshape to (samples, time steps, features) format (LSTM input requirement)\n",
    "X_train_lstm = np.transpose(data_list, (1, 0, 2))\n",
    "y_train = y  # Target variable (standardized yield)\n",
    "\n",
    "##################Create LSTM Model and Perform Hyperparameter Tuning####################################\n",
    "# Define storage path for tuning results (SQLite database, stored in the code directory)\n",
    "storage_name = f\"sqlite:///{country}_{modelname}_region{region}.db\"\n",
    "study_name = f\"{country}_{modelname}_region{region}\"  # Name of the Optuna study\n",
    "\n",
    "# Create Optuna study: minimize loss, use TPE sampler, load existing study if it exists\n",
    "study = optuna.create_study(\n",
    "    direction='minimize',\n",
    "    sampler=optuna.samplers.TPESampler(),\n",
    "    study_name=study_name,\n",
    "    storage=storage_name,\n",
    "    load_if_exists=True\n",
    ")\n",
    "\n",
    "# Run hyperparameter optimization: 200 trials, 4 parallel jobs\n",
    "study.optimize(objective_LSTM, n_trials=200, n_jobs=4)\n",
    "\n",
    "# Output best hyperparameters and corresponding minimum loss\n",
    "print('Best parameters: ', study.best_params)\n",
    "print('Best score (mean MSE): ', study.best_value)\n",
    "\n",
    "# Save best hyperparameters to JSON file\n",
    "save_dir = os.path.join(inputpath_base, 'best_params')\n",
    "os.makedirs(save_dir, exist_ok=True)  # Create directory if it doesn't exist\n",
    "save_path = os.path.join(save_dir, f\"{study_name}_best_params.json\")\n",
    "with open(save_path, \"w\") as f:\n",
    "    json.dump(study.best_params, f, indent=4)  # Save with indentation for readability\n",
    "print(\"Best hyperparameters have been saved to JSON file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab0d7cd-a84f-4ce7-8525-0cbe5b929168",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd4d432-6d9b-4875-a47f-6176032ba160",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8798c657-b302-4bb4-a4d3-a0770b994c72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffc2bc8-ea10-44cf-9312-88fe647f87b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b50b479-356a-4322-b2d6-1e8e45abfaa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb31269c-352b-4753-8306-757950eacbdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee4a32c-b922-44d4-ade1-ead8f30a4b23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8d84cb-14ed-42c7-8ae1-19da40105798",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c83dd3-f6ba-4b1b-bc9b-a7e85fb13530",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e583332-37a4-4610-b767-be77c7a7a9e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6236a3c3-7265-4b8d-ad24-04921995f556",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8545f4-9969-4340-a653-bd15138de58a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320d5529-6799-485f-a848-20b67a39d203",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8703b43f-4da6-4f4a-af70-f9d414ac35f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae3567c-e146-4e4d-9977-1ea6598f286b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53172a18-dcb5-4785-8d6e-d012ea2a10f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5802dda5-1761-4d05-b67e-a68d833400b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80d002c-e2a0-459f-a687-88374490cc24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9a3f15-bc58-4ce4-ad21-49ff1a1f2a28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a89ad8-28bd-4091-9d51-82ea676ac2e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7184a029-2528-43c9-af2c-3d8b0bbab583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaa5d71-aec9-41e0-b461-be518c9741cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c18c92-078b-484c-ae68-b6ff79d7d7d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abad2c0-8a59-4261-b0d3-6a2a3309b856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a29f181-89c4-427c-9da7-744cb5ec9fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc0f25b-cd92-482e-8164-db64e01a17d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e1ac82-841a-4d77-89c4-056e3f22e5d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c0a117-0431-4d73-8015-d61122649ab0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179f21fb-fee1-4fb8-92ec-cdef40e8fe69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e153cebd-6b35-4efb-970d-370a42327e8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05078a10-2173-4f83-a9b0-40c330db7233",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a531a1c-9b44-4aba-b2af-bd39b024bdff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9053c05a-5981-4768-933a-3b2db57d3473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a73b85-fdfb-4795-b711-fe7dc2d8d4f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e91ac12-9968-4928-a9c4-9f54fce07ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23f7353-cfcb-497b-8cf9-599b76c87119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d39cb7-cf19-446e-8c9b-e14520ae7654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242565c3-7f71-46d9-aad1-a3f406c6a3a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68a0173-42f6-4b1b-ae84-39aa8f074f60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fec3dfb-0b9d-40fe-9f69-78199076b212",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbf59e4-f57d-4a47-bb68-c55d11d501ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e75b1c1-5f23-47b0-89bf-86f2ac53e2a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1f2abb-5943-4047-9fc2-abf006c74e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fb1052-c3f9-48ef-9fc0-58a3c26b4f2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897c2cab-20e1-45dd-8c37-d0ac56d72b56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2c86bd-66c2-4602-b6d4-d757af8fc502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5323ac9c-c2ff-42c4-a6b4-7976cb226708",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8847819-d127-4c5d-9874-93c73d108a80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21a42d5-2fcb-4343-adea-889fbb33fe5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfa5154-d0e6-42f5-a095-c4e5053e228f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4425836-86a2-4a73-95e1-1b744d430e5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddf03a6-4985-4f16-85ea-f5b9e2cc31ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1d5783-426c-4e80-a858-0779d809a9ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf713424-19bc-4e3d-9b21-047a2f6933b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c4f84f-a4c1-4eaf-8171-9bf57d6f048c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edf54ae-0b17-4992-93fb-a3c0a7909b10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28efc902-d00c-49be-9388-a9c28d0f2de4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
